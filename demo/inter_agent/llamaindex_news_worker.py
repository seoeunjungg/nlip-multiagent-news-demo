"""
LlamaIndex Agent for Inter-Agent Communication Demo

This agent executes the actual tech news tool implementations and serves
requests delegated from the LangChain agent via NLIP protocol.
"""

import asyncio
import os
from typing import Any, Dict, List, Optional
from dotenv import load_dotenv

from llama_index.core.tools import FunctionTool
from llama_index.core.agent.workflow import FunctionAgent
from llama_index.core.workflow import Context
from llama_index.llms.openai import OpenAI

# Import NLIP components
from nlip_sdk.nlip import NLIP_Factory
from nlip_sdk import nlip
from nlip_server import server

# Import shared utilities
from ..shared.news_tools import get_tech_news_brief

# Load environment variables
load_dotenv()


class LlamaIndexApplication(server.NLIP_Application):
    """LlamaIndex application for inter-agent communication."""
    
    async def startup(self):
        print("üîß Starting LlamaIndex Agent...")
        print("This agent executes tech news tools for requests delegated via NLIP protocol")

    async def shutdown(self):
        print("üõë Shutting down LlamaIndex Agent")
        return None

    async def create_session(self) -> server.NLIP_Session:
        return LlamaIndexSession()


class LlamaIndexSession(server.NLIP_Session):
    """Chat session using LlamaIndex with actual tool implementations."""
    
    def __init__(self):
        super().__init__()
        self.llm = None
        self.agent = None
        self.tools = []
        self.context = None

    async def start(self):
        """Initialize LlamaIndex components for tool execution."""
        try:
            print("üîß Initializing LlamaIndex components...")

            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY env var is required")
            
            print(f"üîë Using OpenAI API key: {api_key[:10]}...")
            
            self.llm = OpenAI(
                model="gpt-4o-mini",
                api_key=os.getenv("OPENAI_API_KEY"),
                temperature=0.3,
                max_tokens=512,
            )
            
            # Create FunctionTool objects from our async functions
            self.tools = [
                FunctionTool.from_defaults(
                    fn=get_tech_news_brief,
                    name="get_tech_news_brief",
                    description=(
                        "Fetch recent technology/AI/cybersecurity news about a topic in the last few days. "
                        "Args: topic (str), days (int, optional)."
                    ),
                ),
            ]
            
            # Create LlamaIndex FunctionAgent
            self.agent = FunctionAgent(
                tools=self.tools,
                llm=self.llm,
                verbose=True,
                system_prompt=(
                    "You are a specialized tech news agent. You fetch and summarize recent news about "
                    "computer science, AI, and cybersecurity. You have a tool that retrieves recent "
                    "articles (titles, sources, publication dates, short descriptions, URLs). "
                    "Use that tool to collect information, then synthesize concise, factual summaries. "
                    "Do not invent sources; if information is missing or uncertain, say so explicitly."
                )
            )

            
            # Initialize context for maintaining conversation state
            self.context = Context(self.agent)
            
            print("‚úÖ LlamaIndex components initialized successfully.")
            print(f"üõ†Ô∏è Available tech news tools: {[tool.metadata.name for tool in self.tools]}")
            
        except Exception as e:
            print(f"‚ùå Error initializing LlamaIndex components: {e}")
            raise

    async def execute(self, msg: nlip.NLIP_Message) -> nlip.NLIP_Message:
        """Execute delegated query using LlamaIndex agent with real tools."""
        logger = self.get_logger()
        text = msg.extract_text()
        
        try:
            print(f"\nüîß [LlamaIndex] Processing delegated query: {text}")
            print("=" * 80)
            
            # Reset context for fresh conversation
            self.context = Context(self.agent)
            
            # Use the agent to process the query
            response = await self.agent.run(text, ctx=self.context)
            response_text = str(response)
            
            print("=" * 80)
            print(f"‚úÖ [LlamaIndex] Completed processing, returning result to coordinator\n")
            logger.info(f"LlamaIndex Response: {response_text}")
            return NLIP_Factory.create_text(response_text)
            
        except Exception as e:
            logger.error(f"Exception in LlamaIndex execution: {e}")
            return NLIP_Factory.create_text(f"‚ùå Error processing delegated request: {str(e)}")

    async def stop(self):
        """Clean up resources."""
        print("üõë Stopping LlamaIndex session")

    async def stop(self):
        """Clean up resources."""
        print("üõë Stopping LlamaIndex worker session")
        self.llm = None
        self.agent = None
        self.tools = []
        self.context = None

app = server.setup_server(LlamaIndexApplication())
